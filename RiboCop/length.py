"""Utilities for translating ORF detection
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import warnings

from collections import Counter
from collections import defaultdict

import pysam
from tqdm import *
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import numpy as np
import pandas as pd

from .fasta import FastaReader
from .gtf import GTFReader
from .interval import Interval
from .common import is_read_uniq_mapping
from .common import merge_intervals
from .common import cal_periodicity
from .infer_protocol import infer_protocol


class PutativeORF:
    """Class for putative ORF."""

    def __init__(self,
                 category,
                 transcript_id,
                 transcript_type,
                 gene_id,
                 gene_name,
                 gene_type,
                 chrom,
                 strand,
                 intervals,
                 seq='',
                 leader='',
                 trailer=''):
        self.category = category
        self.tid = transcript_id
        self.ttype = transcript_type
        self.gid = gene_id
        self.gname = gene_name
        self.gtype = gene_type
        self.chrom = chrom
        self.strand = strand
        self.intervals = sorted(intervals, key=lambda x: x.start)
        start = self.intervals[0].start
        end = self.intervals[-1].end
        self.seq = seq
        self.oid = '{}_{}_{}_{}'.format(transcript_id, start, end, len(seq))
        self.leader = leader
        self.trailer = trailer

    def __hash__(self):
        return hash(self.oid)

    @property
    def start_codon(self):
        if len(self.seq) < 3:
            return None
        return self.seq[:3]

    @classmethod
    def from_string(cls, line):
        """
        Parameters
        ----------
        line: string
              line for annotation file generated by prepare_orfs
        """
        if not line:
            print('annotation line cannot be empty')
            return None
        fields = line.split('\t')
        if len(fields) != 13:
            print('unexpected number of columns found for annotation file')
            return None
        oid = fields[0]
        category = fields[1]
        tid = fields[2]
        ttype = fields[3]
        gid = fields[4]
        gname = fields[5]
        gtype = fields[6]
        chrom = fields[7]
        strand = fields[8]
        coordinate = fields[9]
        intervals = []
        for group in coordinate.split(','):
            start, end = group.split('-')
            start = int(start)
            end = int(end)
            intervals.append(Interval(chrom, start, end, strand))
        seq = fields[10]
        leader = fields[11]
        trailer = fields[12]
        return cls(category, tid, ttype, gid, gname, gtype, chrom, strand,
                   intervals)

    @classmethod
    def from_tracks(cls, tracks, category, seq='', leader='', trailer=''):
        """
        Parameters
        ----------
        tracks: list of GTFTrack
        """
        if not tracks:
            return None
        intervals = []
        tid = set()
        ttype = set()
        gid = set()
        gname = set()
        gtype = set()
        chrom = set()
        strand = set()
        for track in tracks:
            try:
                tid.add(track.transcript_id)
                ttype.add(track.transcript_type)
                gid.add(track.gene_id)
                gname.add(track.gene_name)
                gtype.add(track.gene_type)
                chrom.add(track.chrom)
                strand.add(track.strand)
                intervals.append(
                    Interval(track.chrom, track.start, track.end,
                             track.strand))
            except AttributeError:
                print('missing attribute {}:{}-{}'.format(
                    track.chrom, track.start, track.end))
                return None
        if (len(tid) != 1 or len(ttype) != 1 or len(gid) != 1
                or len(gname) != 1 or len(gtype) != 1 or len(chrom) != 1
                or len(strand) != 1):
            print('inconsistent tracks for one ORF')
            return None
        tid = list(tid)[0]
        ttype = list(ttype)[0]
        gid = list(gid)[0]
        gname = list(gname)[0]
        gtype = list(gtype)[0]
        chrom = list(chrom)[0]
        strand = list(strand)[0]
        return cls(category, tid, ttype, gid, gname, gtype, chrom, strand,
                   intervals, seq, leader, trailer)


def prepare_orfs(gtf):
    """
    Parameters
    ----------
    gtf: GTFReader
         instance of GTFReader

    Returns
    -------
    cds: List[PutativeORF]
         list of CDS
    """

    if not isinstance(gtf, GTFReader):
        gtf = GTFReader(gtf)

    print('preparing putative ORFs...')

    ### process CDS gtf
    print('searching cds...')
    cds_orfs = []
    for gid in tqdm(gtf.cds):
        for tid in gtf.cds[gid]:
            tracks = gtf.cds[gid][tid]
            orf = PutativeORF.from_tracks(tracks, 'CDS')
            if orf:
                cds_orfs.append(orf)

    return cds_orfs


def split_bam(bam, protocol, prefix):
    """Split bam by read length and strand

    Parameters
    ----------
    bam : str
          Path to bam file
    protocol: str
          Experiment protocol [forward, reverse]
    prefix: str
            prefix for output files

    Returns
    -------
    alignments: dict(dict(Counter))
                bam split by length, strand, (chrom, pos)
    read_lengths: dict
                  key is the length, value is the number of reads
    """
    alignments = defaultdict(lambda: defaultdict(Counter))
    read_lengths = defaultdict(int)
    total_count = qcfail = duplicate = secondary = unmapped = multi = valid = 0
    print('reading bam file...')
    bam = pysam.AlignmentFile(bam, 'rb')
    for r in tqdm(bam.fetch(until_eof=True)):

        total_count += 1

        if r.is_qcfail:
            qcfail += 1
            continue
        if r.is_duplicate:
            duplicate += 1
            continue
        if r.is_secondary:
            secondary += 1
            continue
        if r.is_unmapped:
            unmapped += 1
            continue
        if not is_read_uniq_mapping(r):
            multi += 1
            continue

        map_strand = '-' if r.is_reverse else '+'
        ref_positions = r.get_reference_positions()
        strand = None
        pos = None
        chrom = r.reference_name
        # length = r.query_length
        length = len(ref_positions)
        if protocol == 'forward':
            if map_strand == '+':
                strand = '+'
                pos = ref_positions[0]
            else:
                strand = '-'
                pos = ref_positions[-1]
        elif protocol == 'reverse':
            if map_strand == '+':
                strand = '-'
                pos = ref_positions[-1]
            else:
                strand = '+'
                pos = ref_positions[0]
        # convert bam coordinate to one-based
        alignments[length][strand][(chrom, pos + 1)] += 1
        read_lengths[length] += 1

        valid += 1

    summary = ('summary:\n\ttotal_reads: {}\n\tunique_mapped: {}\n'
               '\tqcfail: {}\n\tduplicate: {}\n\tsecondary: {}\n'
               '\tunmapped:{}\n\tmulti:{}\n\nlength dist:\n').format(
                   total_count, valid, qcfail, duplicate, secondary, unmapped,
                   multi)

    for length in read_lengths:
        summary += '\t{}: {}\n'.format(length, read_lengths[length])

    with open('{}_bam_summary.txt'.format(prefix), 'w') as output:
        output.write(summary)

    return (alignments, read_lengths)


def orf_coverage_length(orf, alignments, length, offset_5p=20, offset_3p=0):
    """
    Parameters
    ----------
    orf: PutativeORF
         instance of PutativeORF
    alignments: dict(dict(Counter))
                alignments summarized from bam
    length: int
            the target length
    offset_5p: int
               the number of nts to include from 5'prime
    offset_3p: int
               the number of nts to include from 3'prime

    Returns
    -------
    nreads: int
            number of reads from this length for the ORF
    """
    nreads = 0
    chrom = orf.chrom
    strand = orf.strand
    if strand == '-':
        offset_5p, offset_3p = offset_3p, offset_5p
    first, last = orf.intervals[0], orf.intervals[-1]
    for pos in range(first.start - offset_5p, first.start):
        try:
            nreads += alignments[length][strand][(chrom, pos)]
        except KeyError:
            pass

    for iv in orf.intervals:
        for pos in range(iv.start, iv.end + 1):
            try:
                nreads += alignments[length][strand][(chrom, pos)]
            except KeyError:
                pass

    for pos in range(last.end + 1, last.end + offset_3p + 1):
        try:
            nreads += alignments[length][strand][(chrom, pos)]
        except KeyError:
            pass

    return nreads


def metagene_coverage(cds, alignments, read_lengths, offset_5p=12,
                      offset_3p=0):
    """
    Parameters
    ----------
    cds: List[PutativeORF]
         list of cds
    alignments: dict(dict(Counter))
                alignments summarized from bam
    read_lengths: dict
                  key is the length, value is the number reads
    prefix: str
            prefix for the output file
    max_positions: int
                   the number of nts to include
    offset_5p: int
               the number of nts to include from the 5'prime
    offset_3p: int
               the number of nts to include from the 3'prime

    Returns
    -------
    metagenes: dict
               key is the length, value is the metagene coverage
    """
    print('calculating metagene profiles...')
    metagenes = defaultdict(defaultdict)
    for orf in tqdm(cds):
        for length in tqdm(read_lengths):
            coverage = orf_coverage_length(orf, alignments, length, offset_5p,
                                           offset_3p)
            metagenes[orf][length] = coverage
    return metagenes


def plot_read_lengths(read_lengths, prefix):
    """
    Parameters
    ----------
    read_lengths: dict
                  key is the length, value is the number of reads
    prefix: str
            prefix for the output file
    """
    print('plotting read length distribution...')
    fig, ax = plt.subplots()
    x = sorted(read_lengths.keys())
    y = [read_lengths[i] for i in x]
    ax.bar(x, y)
    ax.set_xlabel('Read length')
    ax.set_ylabel('Number of reads')
    ax.set_title('Read length distribution')
    fig.tight_layout()
    fig.savefig('{}_read_length_dist.pdf'.format(prefix))
    plt.close()


def genewise_length(bam, gtf, prefix):
    """
    Parameters
    ----------
    gtf: str
         Path to the GTF file
    fasta: str
           Path to the FASTA file
    bam: str
         Path to the bam file
    prefix: str
            prefix for all output files
    annotation: str
                Path for annontation files of putative ORFs
                It will be automatically generated if None
    protocol: str
              'forward' for stranded, 'reverse' for reverse stranded
              It will be automatically inferred if None
    """

    gtf = GTFReader(gtf)
    cds = prepare_orfs(gtf)
    protocol = infer_protocol(bam, gtf, prefix)
    alignments, read_lengths = split_bam(bam, protocol, prefix)
    plot_read_lengths(read_lengths, prefix)
    metagenes = metagene_coverage(cds, alignments, read_lengths)
    col_lens = '\t'.join(['len_{}'.format(i) for i in sorted(read_lengths)])
    to_write = 'ORF_id\tgene_id\tgene_name\tcount\t' + col_lens + '\n'
    formatter = '{}\t{}\t{}\t{}\t' + '{}\t' * (len(read_lengths) - 1) + '{}\n'
    for orf in metagenes:
        d = {}
        for length in read_lengths:
            d[length] = 0
        for length in metagenes[orf]:
            d[length] = metagenes[orf][length]
        reads = [d[i] for i in sorted(d)]
        data = (orf.oid, orf.gid, orf.gname, sum(reads)) + tuple(reads)
        to_write += formatter.format(*data)

    with open('{}_genewise_length.tsv'.format(prefix), 'w') as output:
        output.write(to_write)
