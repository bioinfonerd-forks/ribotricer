"""Utilities for translating ORF detection"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import warnings

from collections import Counter
from collections import defaultdict

import datetime
import numpy as np
from tqdm import *
import pandas as pd

from .bam import split_bam
from bx.intervals.intersection import IntervalTree
from .const import CUTOFF
from .fasta import FastaReader
from .gtf import GTFReader
from .infer_protocol import infer_protocol
from .metagene import metagene_coverage
from .metagene import align_metagenes
from .orf import ORF
from .plotting import plot_read_lengths
from .plotting import plot_metagene
from .prepare_orfs import prepare_orfs
from .statistics import coherence


def merge_read_lengths(alignments, psite_offsets):
    """
    Parameters
    ----------
    alignments: dict(dict(Counter))
                bam split by length, strand
    psite_offsets: dict
                   key is the length, value is the offset
    Returns
    -------
    merged_alignments: dict(dict)
                       alignments by merging all lengths
    """
    # print('merging different lengths...')
    merged_alignments = defaultdict(Counter)

    for length, offset in psite_offsets.items():
        for strand in alignments[length]:
            for chrom, pos in alignments[length][strand]:
                count = alignments[length][strand][(chrom, pos)]
                if strand == '+':
                    pos_shifted = pos + offset
                else:
                    pos_shifted = pos - offset
                merged_alignments[strand][(chrom, pos_shifted)] += count
    return merged_alignments


def parse_ribocop_index(ribocop_index):
    """
    Parameters
    ----------
    ribocop_index: str
                   Path to the index file generated by RiboCop prepare_orfs

    Returns
    -------
    annotated: List[ORF]
               ORFs of CDS annotated
    novel: List[ORF]
           list of non-annotated ORFs
    refseq: defaultdict(IntervalTree)
            chrom: (start, end, strand)
    """

    annotated = []
    novel = []
    refseq = defaultdict(IntervalTree)

    # print('parsing candidate ORFs...')
    with open(ribocop_index, 'r') as anno:
        total_lines = len(['' for line in anno])
    with open(ribocop_index, 'r') as anno:
        with tqdm(total=total_lines) as pbar:
            header = True
            for line in anno:
                pbar.update()
                if header:
                    header = False
                    continue
                orf = ORF.from_string(line)
                if orf is None:
                    continue
                refseq[orf.chrom].insert(orf.intervals[0].start,
                                         orf.intervals[-1].end, orf.strand)
                if orf.category == 'annotated':
                    annotated.append(orf)
                else:
                    novel.append(orf)
    return (annotated, novel, refseq)


def orf_coverage(orf, alignments, offset_5p=20, offset_3p=0):
    """
    Parameters
    ----------
    orf: ORF
         instance of ORF
    alignments: dict(Counter)
                alignments summarized from bam by merging lengths
    offset_5p: int
               the number of nts to include from 5'prime
    offset_3p: int
               the number of nts to include from 3'prime

    Returns
    -------
    coverage: Series
              coverage for ORF for specific length
    """
    coverage = []
    chrom = orf.chrom
    strand = orf.strand
    if strand == '-':
        offset_5p, offset_3p = offset_3p, offset_5p
    first, last = orf.intervals[0], orf.intervals[-1]
    for pos in range(first.start - offset_5p, first.start):
        try:
            coverage.append(alignments[strand][(chrom, pos)])
        except KeyError:
            coverage.append(0)

    for iv in orf.intervals:
        for pos in range(iv.start, iv.end + 1):
            try:
                coverage.append(alignments[strand][(chrom, pos)])
            except KeyError:
                coverage.append(0)

    for pos in range(last.end + 1, last.end + offset_3p + 1):
        try:
            coverage.append(alignments[strand][(chrom, pos)])
        except KeyError:
            coverage.append(0)

    if strand == '-':
        coverage.reverse()
        return pd.Series(
            np.array(coverage),
            index=np.arange(-offset_3p,
                            len(coverage) - offset_3p))
    else:
        return pd.Series(
            np.array(coverage),
            index=np.arange(-offset_5p,
                            len(coverage) - offset_5p))


def export_orf_coverages(orfs, merged_alignments, prefix, report_all=False):
    """
    Parameters
    ----------
    orfs: List[ORF]
          a list of candidate orfs
    merged_alignments: dict(dict)
                       alignments by merging all lengths
    prefix: str
            prefix for output file
    report_all: bool
                if True, all coverages will be exported
    """
    # print('exporting coverages for all ORFs...')
    to_write = 'ORF_ID\tstatus\tphase_score\tread_count\tlength\tvalid_codons\tprofile\n'
    for orf in tqdm(orfs):
        oid = orf.oid
        cov = orf_coverage(orf, merged_alignments)
        cov = cov.astype(int)
        cov = cov.tolist()
        count = sum(cov)
        length = len(cov)
        coh, valid = coherence(cov)
        status = 'translating' if coh >= CUTOFF else 'nontranslating'
        if not report_all and coh < CUTOFF:  # skip those fail the cutoff
            continue
        to_write += '{}\t{}\t{}\t{}\t{}\t{}\t{}\n'.format(
            oid, status, coh, count, length, valid, cov)
    with open('{}_translating_ORFs.tsv'.format(prefix), 'w') as output:
        output.write(to_write)


def export_wig(merged_alignments, prefix):
    """
    Parameters
    ----------
    merged_alignments: dict(dict)
                       alignments by merging all lengths
    prefix: str
            prefix of output wig files
    """
    # print('exporting merged alignments to wig file...')
    for strand in merged_alignments:
        to_write = ''
        cur_chrom = ''
        for chrom, pos in sorted(merged_alignments[strand]):
            if chrom != cur_chrom:
                cur_chrom = chrom
                to_write += 'variableStep chrom={}\n'.format(chrom)
            to_write += '{}\t{}\n'.format(
                pos, merged_alignments[strand][(chrom, pos)])
        if strand == '+':
            fname = '{}_pos.wig'.format(prefix)
        else:
            fname = '{}_neg.wig'.format(prefix)
        with open(fname, 'w') as output:
            output.write(to_write)


def detect_orfs(bam, ribocop_index, prefix, stranded, read_lengths,
                psite_offsets, report_all):
    """
    Parameters
    ----------
    bam: str
         Path to the bam file
    ribocop_index: str
                   Path to the index file generated by RiboCop prepare_orfs
    prefix: str
            prefix for all output files
    stranded: str
              {'forward', 'no', 'reverse'}
              If None, the strandedness will be automatically inferred
    read_lengths: list[int]
                  read lengths to use
                  If None, it will be automatically determined by assessing
                  the periodicity of metagene profile of this read length
    psite_offsets: dict
                   Psite offsets for each read lengths
                   If None, the profiles from different read lengths will be
                   automatically aligned using cross-correlation
    report_all: bool
                Whether to output all ORFs' scores regardless of translation
                status
    """
    now = datetime.datetime.now()
    print(now.strftime('%b %d %H:%M:%S ..... started RiboCop detect-orfs'))

    ### parse the index file
    now = datetime.datetime.now()
    print(
        now.strftime('%b %d %H:%M:%S ... started parsing RiboCop index file'))
    annotated, novel, refseq = parse_ribocop_index(ribocop_index)

    ### infer experimental protocol if not provided
    if stranded is None:
        now = datetime.datetime.now()
        print('{} ... {}'.format(
            now.strftime('%b %d %H:%M:%S'),
            'started inferring experimental design'))
        stranded = infer_protocol(bam, refseq, prefix)

    ### split bam file into strand and read length
    now = datetime.datetime.now()
    print(now.strftime('%b %d %H:%M:%S ... started reading bam file'))
    alignments, read_length_counts = split_bam(bam, protocol, prefix,
                                               read_lengths)

    ### plot read length distribution
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'),
        'started plotting read length distribution'))
    plot_read_lengths(read_length_counts, prefix)

    ### calculate metagene profiles
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'),
        'started calculating metagene profiles. This may take a long time...'))
    metagenes = metagene_coverage(annotated, alignments, read_length_counts,
                                  prefix)

    ### plot metagene profiles
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'), 'started plotting metagene profiles'))
    plot_metagene(metagenes, read_length_counts, prefix)

    ### align metagenes if psite_offsets not provided
    if psite_offsets is None:
        now = datetime.datetime.now()
        print('{} ... {}'.format(
            now.strftime('%b %d %H:%M:%S'),
            'started inferring P-site offsets'))
        psite_offsets = align_metagenes(metagenes, read_length_counts, prefix,
                                        read_lengths is None)

    ### merge read lengths based on P-sites offsets
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'),
        'started shifting according to P-site offsets'))
    merged_alignments = merge_read_lengths(alignments, psite_offsets)

    ### export wig file
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'),
        'started exporting wig file of alignments after shifting'))
    export_wig(merged_alignments, prefix)

    ### saving detecting results to disk
    now = datetime.datetime.now()
    print('{} ... {}'.format(
        now.strftime('%b %d %H:%M:%S'), 'started saving results into disk'))
    export_orf_coverages(annotated + novel, merged_alignments, prefix,
                         report_all)
